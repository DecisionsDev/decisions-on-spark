{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calling RES with Python in SPARK\n",
    "## Pre-Requisite\n",
    "   * PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/spark-2.4.0-bin-hadoop2.7/python (2.4.0)\n",
      "Collecting py4j==0.10.7 (from pyspark)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
      "\u001b[K    100% |████████████████████████████████| 204kB 2.4MB/s ta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: py4j\n",
      "Successfully installed py4j-0.10.7\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Python environment with ODM Jars files and ODM Model archive\n",
    "\n",
    "   * Create a Spark Session\n",
    "   * Initialize the Python environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  4146  100  4146    0     0  14913      0 --:--:-- --:--:-- --:--:-- 14913\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 14663  100 14663    0     0  61609      0 --:--:-- --:--:-- --:--:-- 61869\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 32796  100 32796    0     0   310k      0 --:--:-- --:--:-- --:--:--  310k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 13.7M  100 13.7M    0     0  2243k      0  0:00:06  0:00:06 --:--:-- 2319k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 8918k  100 8918k    0     0  2190k      0  0:00:04  0:00:04 --:--:-- 2190k\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://4aff01ac99a1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb17df2fe80>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from io import StringIO\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "import os\n",
    "\n",
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "# Download Material for Rule Exection\n",
    "!curl -o {cwd}/miniloan-xom.jar https://raw.githubusercontent.com/ODMDev/decisions-on-spark/pythonintegration/data/miniloan/miniloan-xom.jar\n",
    "!curl -o {cwd}/miniloan-ruleapp.jar https://raw.githubusercontent.com/ODMDev/decisions-on-spark/pythonintegration/data/miniloan/miniloan-ruleapp.jar\n",
    "# Download ODM Library\n",
    "!curl -o {cwd}/j2ee_connector-1_5-fr.jar http://159.122.179.123:31329/download/lib/ODM8920/j2ee_connector-1_5-fr.jar\n",
    "!curl -o {cwd}/jrules-engine.jar http://159.122.179.123:31329/download/lib/ODM8920/jrules-engine.jar\n",
    "!curl -o {cwd}/jrules-res-execution.jar http://159.122.179.123:31329/download/lib/ODM8920/jrules-res-execution-memory.jar\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--jars local:\"+cwd+\"/miniloan-ruleapp.jar,local:\"+cwd+\"/miniloan-xom.jar,local:\"+cwd+\"/jrules-engine.jar,local:\"+cwd+\"/j2ee_connector-1_5-fr.jar,local:\"+cwd+\"/jrules-res-execution.jar pyspark-shell\"\n",
    "\n",
    "\n",
    "import pyspark # only run after findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark Session\n",
    "sc = SparkSession.builder.getOrCreate()\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- creditScore: long (nullable = true)\n",
      " |-- income: long (nullable = true)\n",
      " |-- loanAmount: long (nullable = true)\n",
      " |-- monthDuration: long (nullable = true)\n",
      " |-- rate: double (nullable = true)\n",
      " |-- yearlyReimbursement: long (nullable = true)\n",
      " |-- default: long (nullable = true)\n",
      "\n",
      "+--------+-----------+------+----------+-------------+----+-------------------+-------+\n",
      "|    name|creditScore|income|loanAmount|monthDuration|rate|yearlyReimbursement|default|\n",
      "+--------+-----------+------+----------+-------------+----+-------------------+-------+\n",
      "|John Doe|        436|290532|    136331|          240|0.08|              13979|      0|\n",
      "|John Doe|        701| 94722|    150099|          120|0.08|              15091|      0|\n",
      "|John Doe|        670| 86878|    269819|          180|0.08|              26947|      1|\n",
      "|John Doe|        717|274553|    513754|           36|0.08|              53720|      0|\n",
      "|John Doe|        471|278339|    206578|          120|0.07|              24874|      0|\n",
      "|John Doe|        827|230820|    757193|          100|0.08|              77498|      1|\n",
      "|John Doe|        620|103873|    237225|           80|0.08|              22918|      0|\n",
      "|John Doe|        475| 78719|    209293|          240|0.06|              28607|      0|\n",
      "|John Doe|        625|264765|     86406|          120|0.07|               8948|      0|\n",
      "|John Doe|        621|183986|    307994|          120|0.08|              29922|      0|\n",
      "+--------+-----------+------+----------+-------------+----+-------------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a SParkSQL Context to load the data in a dataframe\n",
    "sql = SQLContext(sc.sparkContext)\n",
    "new_decisions_pd = pd.read_csv(\"https://raw.githubusercontent.com/ODMDev/decisions-on-spark/master/data/miniloan/miniloan-decisions-defaultly-1K.csv\")\n",
    "request_df = sql.createDataFrame(new_decisions_pd)\n",
    "request_df.printSchema()\n",
    "request_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution fininsh\n"
     ]
    }
   ],
   "source": [
    "from  pyspark.sql import Row\n",
    "\n",
    "def CreateODMSession(sc):\n",
    "    if not hasattr(CreateODMSession, \"fac\"):\n",
    "        factoryConfig = sc._jvm.ilog.rules.res.session.IlrJ2SESessionFactory.createDefaultConfig()\n",
    "        xuConfig = factoryConfig.getXUConfig();\n",
    "        xuConfig.setLogAutoFlushEnabled(True);\n",
    "        xuConfig.getPersistenceConfig().setPersistenceType(sc._jvm.ilog.rules.res.session.config.IlrPersistenceType.MEMORY);\n",
    "        xuConfig.getManagedXOMPersistenceConfig().setPersistenceType(sc._jvm.ilog.rules.res.session.config.IlrPersistenceType.MEMORY);\n",
    "        CreateODMSession.fac=sc._jvm.ilog.rules.res.session.IlrJ2SESessionFactory(factoryConfig)    \n",
    "    return CreateODMSession.fac\n",
    "\n",
    "def execute (row):\n",
    "    sc = SparkSession.builder.getOrCreate()\n",
    "    factory=CreateODMSession(sc)\n",
    "\n",
    "    sessionRequest = factory.createRequest()\n",
    "    sessionRequest.setRulesetPath(sc._jvm.ilog.rules.res.model.IlrPath.parsePath(\"/miniloanruleapp/miniloanrules\"))\n",
    "    # Ensure latest version of the ruleset is taken into account\n",
    "    sessionRequest.setForceUptodate(True)\n",
    "    # Set the input parameters for the execution of the rules\n",
    "    inputParameters = sc._jvm.java.util.HashMap()\n",
    "    borrower =  sc._jvm.miniloan.Borrower(row.name, row.creditScore,row.income)\n",
    "    loan =  sc._jvm.miniloan.Loan()\n",
    "    loan.setAmount(row.loanAmount)\n",
    "    loan.setDuration(row.monthDuration)\n",
    "    loan.setYearlyInterestRate(row.rate)\n",
    "    \n",
    "    # Set parameters\n",
    "    inputParameters[\"loan\"]=loan\n",
    "    inputParameters[\"borrower\"]=borrower\n",
    "\n",
    "    sessionRequest.setInputParameters(inputParameters)\n",
    "    session = factory.createStatelessSession()\n",
    "    # Perfrom ODM Execution \n",
    "    response = session.execute(sessionRequest)\n",
    "    col= response.getOutputParameters()\n",
    "#    for key in col:\n",
    "#        print (key, \"corresponds to\", col[key])\n",
    "    loanResult= response.getOutputParameters().get(\"loan\")\n",
    "    return Row(isApproved=loanResult.isApproved(),firedRulesCount=col['ilog.rules.firedRulesCount'])\n",
    "#execute(\"dd\")\n",
    "\n",
    "dfResult = request_df.rdd.map(execute).toDF()\n",
    "count= dfResult.count()\n",
    "print(\"Execution fininsh\")\n",
    "#rddResult\n",
    "# Count the nb of Loan approved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can query execution Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfResult.createOrReplaceTempView(\"loan\")\n",
    "sc.sql(\"SELECT isApproved FROM loan\").show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
